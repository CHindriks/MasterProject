{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b18a39f",
   "metadata": {},
   "source": [
    "# PDF & CDF & KS-Test Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b16495",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "- time_series: The observing times\n",
    "- flare_time:  The flaring times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3e91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "\n",
    "import math\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams, font_manager as fm\n",
    "from astropy.io import fits\n",
    "from scipy.stats import stats, expon, erlang\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm  # progress bar\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import kstest, gamma\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Global Settings ---\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "\n",
    "# --- Load custom font --- \n",
    "custom_font = fm.FontProperties(\n",
    "    fname=\"/usr/share/fonts/dejavu-serif-fonts/DejaVuSerifCondensed.ttf\"\n",
    ")\n",
    "\n",
    "# --- Apply global Matplotlib settings --- \n",
    "plt.rcParams.update({\n",
    "    \"font.family\": custom_font.get_name(),\n",
    "    \"mathtext.fontset\": \"stix\",\n",
    "    \"font.size\": 12,\n",
    "    \"figure.dpi\": 150\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7f3d0",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014f2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_number_of_flares(time_gap_start, time_gap_end, lam):\n",
    "    \"\"\"\n",
    "    Compute the expected number of flares occurring within each observational time gap.\n",
    "\n",
    "    Inputs\n",
    "        time_gap_start (np.ndarray): Array of start times for each gap.\n",
    "        time_gap_end   (np.ndarray): Array of end times for each gap.\n",
    "        lam (float): Mean flaring rate (flares per unit time).\n",
    "\n",
    "    Outputs\n",
    "        np.ndarray or None: Expected number of flares per gap, or None if no gaps exist.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Check if time_gap_start & time_gap_end contain any gaps, if not. Skip function \n",
    "    if (isinstance(time_gap_start, np.ndarray) and isinstance(time_gap_end, np.ndarray) and len(time_gap_start) == 1 and len(time_gap_end) == 1 and time_gap_start[0] == 0 and time_gap_end[0] == 0):\n",
    "        print(\"No gaps in data — skipping expected_number_of_flares function.\")\n",
    "        return  \n",
    "    \n",
    "    assert len(time_gap_start) == len(time_gap_end), f\"The size of the time_gap_start and time_gap_end are not equal {len(time_gap_start), len(time_gap_end)}.\"\n",
    "    \n",
    "    # Find the number of gaps present in the data\n",
    "    number_of_gaps = len(time_gap_start)\n",
    "    \n",
    "    # Calculates the gap time\n",
    "    time_gap_time = time_gap_end - time_gap_start\n",
    "    assert len(time_gap_time) > 0, f\"time_gap_time should contain {number_of_gaps} number of gaps.\"\n",
    "    \n",
    "    # Calculate the expected number of flares in each time_gap_time\n",
    "    expected_flares = lam * time_gap_time\n",
    "    assert len(expected_flares) == len(time_gap_time), f\"The shape of expected_flares {len(expected_flares)} is not the same as the shape of time_gap_time {len(time_gap_time)}\"\n",
    "    \n",
    "    return expected_flares\n",
    "\n",
    "\n",
    "def generate_times(time_gap_start, time_gap_end, lam):\n",
    "    \"\"\"\n",
    "    Generate simulated flare times inside each observational gap.\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    time_gap_start : array-like\n",
    "        Start times of the observational gaps.\n",
    "    time_gap_end : array-like\n",
    "        End times of the observational gaps.\n",
    "    lam : float\n",
    "        Rate parameter (lambda) for the Poisson process.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    flare_times : numpy.ndarray\n",
    "        Array of simulated flare timestamps within the gaps. Returns None if no\n",
    "        expected flares are found.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    expected_flares = expected_number_of_flares(time_gap_start, time_gap_end, lam)\n",
    "        \n",
    "    # Skip if expected_flares is None\n",
    "    if expected_flares is None:\n",
    "        print(\"No expected flares — skipping function.\")\n",
    "        return\n",
    "    \n",
    "    # Calculates the gap time\n",
    "    time_gap_time = time_gap_end - time_gap_start\n",
    "    \n",
    "    # Calculate the number of flares in each gap\n",
    "    number_simulated_flares = np.random.poisson(expected_flares)\n",
    "    \n",
    "    # Pick a time \n",
    "    flare_times = np.concatenate([np.random.uniform(start, end, size=n) for start, end, n in zip(time_gap_start, time_gap_end, number_simulated_flares)])\n",
    "        \n",
    "    return flare_times\n",
    "\n",
    "\n",
    "def bootstrapped_data(number_of_simulations, time_gap_start, time_gap_end, lam, flare_times_observed):\n",
    "    \"\"\"\n",
    "    Generate bootstrapped flare-time datasets by combining simulated and observed flares.\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    number_of_simulations : int\n",
    "        Number of bootstrap datasets to generate.\n",
    "    time_gap_start : array-like\n",
    "        Start times of the observational gaps.\n",
    "    time_gap_end : array-like\n",
    "        End times of the observational gaps.\n",
    "    lam : float\n",
    "        Mean flaring rate parameter (lambda) for the Poisson process.\n",
    "    flare_times_observed : array-like\n",
    "        Array of the flare times observed in the real data.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    flare_times_combined_list of numpy.ndarray\n",
    "        A list where each element is a sorted array of combined observed and\n",
    "        simulated flare times for one bootstrap iteration. Returns flare_times_observed_list if no\n",
    "        gaps are present in the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check the mean flaring rate.\n",
    "    assert isinstance(lam, (float, int)), \"The mean flaring rate should be a float or int\"\n",
    "    assert lam > 0, f\"The mean flaring rate should be positive, currently it is {lam:.2f}\"\n",
    "    \n",
    "    # Check the number_of_simulations.\n",
    "    assert isinstance(number_of_simulations, (float, int)), \"The amount of simulations should be a float or int\"\n",
    "    assert number_of_simulations > 0, f\"The amount of simulations should be positive, currently it is {number_of_simulations:.0f}\"\n",
    "    \n",
    "    # Check if time_gap_start & time_gap_end contain any gaps, if not. Skip function \n",
    "    if (isinstance(time_gap_start, np.ndarray) and isinstance(time_gap_end, np.ndarray) and len(time_gap_start) == 1 and len(time_gap_end) == 1 and time_gap_start[0] == 0 and time_gap_end[0] == 0):\n",
    "        \n",
    "        flare_times_observed_list = []\n",
    "        for n_simulation in range(number_of_simulations):\n",
    "            flare_times_observed_list.append(np.array(flare_times_observed))\n",
    "            \n",
    "        return flare_times_observed_list\n",
    "    \n",
    "    flare_times_combined_list = []\n",
    "    for n_simulation in range(number_of_simulations):\n",
    "        # Generate flare times\n",
    "        flare_times_simulated = generate_times(t_gaps_start, t_gaps_end, lam)\n",
    "        /\n",
    "        # Combine the simulated with the observed flares\n",
    "        flare_times_combined = np.concatenate([flare_times_observed,flare_times_simulated])\n",
    "        \n",
    "        # Sort the flare times\n",
    "        flare_times_combined_sorted = np.sort(flare_times_combined)\n",
    "        \n",
    "        # Append this to the list\n",
    "        flare_times_combined_list.append(flare_times_combined_sorted)\n",
    "        \n",
    "    return flare_times_combined_list\n",
    "\n",
    "def pairwise_waiting_times(timeseries):\n",
    "    \"\"\"\n",
    "    Compute all pairwise waiting times from a sequence of time points.\n",
    "\n",
    "    Given an iterable of time values (e.g., event timestamps), this function\n",
    "    calculates the difference between every pair of times where the second\n",
    "    time occurs after the first. In other words, for each pair (i, j) with\n",
    "    j > i, it computes times[j] - times[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timeseries : iterable of numbers\n",
    "        A sequence of time points (e.g., [t0, t1, t2, ...]) representing when events occur.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of numbers\n",
    "        A list of all positive time differences between pairs of time points.\n",
    "        The list length is n*(n-1)/2 for n input times.\"\"\"\n",
    "    t = list(timeseries)\n",
    "    return [t[j] - t[i] for i in range(len(t)) for j in range(i+1, len(t))]\n",
    "\n",
    "def waiting_time_calculator(number_of_simulations, time_gap_start, time_gap_end, lam, flare_times_observed, stacked):\n",
    "    \"\"\"\n",
    "    Compute waiting-time distributions from bootstrapped flare-time simulations.\n",
    "    \n",
    "    - If `stacked=True`: waiting times are computed using pairwise waiting times.\n",
    "    - If `stacked=False`: waiting times are computed using simple first differences.\n",
    "\n",
    "    Input\n",
    "    ----------\n",
    "    number_of_simulations : int\n",
    "        Number of bootstrapped datasets to generate.\n",
    "    time_gap_start : array-like\n",
    "        Start times of the observational gaps.\n",
    "    time_gap_end : array-like\n",
    "        End times of the observational gaps.\n",
    "    lam : float\n",
    "        Mean flaring rate parameter.\n",
    "    flare_times_observed : array-like\n",
    "        Observed flare timestamps.\n",
    "    stacked : bool\n",
    "        Whether to compute pairwise waiting times (True) or simple differences (False).\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    list of numpy.ndarray\n",
    "        A list where each element contains the waiting times for one simulation.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(stacked, bool), \"stacked must be either True or False\"\n",
    "\n",
    "    # Check the number_of_simulations.\n",
    "    assert isinstance(number_of_simulations, (int)), \"The amount of simulations should be a int\"\n",
    "    assert number_of_simulations > 0, f\"The amount of simulations should be positive, currently it is {number_of_simulations:.0f}\"\n",
    "    assert np.all(np.array(flare_times_observed) >= 0), \"All values in flare_times_observed must be positive\"\n",
    "    \n",
    "    # Calculate the flare times in case there are gaps in the data\n",
    "    flare_times = bootstrapped_data(number_of_simulations, t_gaps_start, t_gaps_end, lam, t_flares)\n",
    "    \n",
    "    # Generate an empty list for all the waiting times.\n",
    "    waiting_time_list = []\n",
    "    number_of_flares_list = []\n",
    "    \n",
    "    # Loop over the number of simulations\n",
    "    for n_simulation in range(number_of_simulations):\n",
    "        \n",
    "        # Calculate the amount of flares\n",
    "        number_of_flares_list.append(len(flare_times[n_simulation]))\n",
    "        \n",
    "        \n",
    "        # Check if stacked is true or false to use a different method for calculating the waiting times. \n",
    "        if stacked == True:\n",
    "            # Subtract the first flare time from all others\n",
    "            waiting_times_subtracted = flare_times[n_simulation]-flare_times[n_simulation][0]\n",
    "            \n",
    "            \n",
    "            waiting_times = pairwise_waiting_times(waiting_times_subtracted)\n",
    "            waiting_time_list.append(waiting_times)\n",
    "        else:\n",
    "            # Subtract the first flare time from all others\n",
    "            waiting_times_subtracted = flare_times[n_simulation]-flare_times[n_simulation][0]\n",
    "            \n",
    "            \n",
    "            waiting_times = np.diff(waiting_times_subtracted)\n",
    "            waiting_time_list.append(waiting_times)\n",
    "            \n",
    "            \n",
    "    return waiting_time_list, number_of_flares_list\n",
    "\n",
    "def pdf_prepper(waiting_time_list, time_gaps_start, binsize):\n",
    "    \"\"\"    \n",
    "    Prepare a probability density function (PDF) from waiting-time data.\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    waiting_time_list : list of array-like\n",
    "        A list of waiting-time sequences.\n",
    "    time_gaps_start : array-like\n",
    "        Indicator of whether gaps exist in the data. np.array([0]) means no gaps.\n",
    "    binsize : float\n",
    "        Desired histogram bin size & domain stepsize.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    counts : numpy.ndarray\n",
    "        Normalized histogram values representing the PDF.\n",
    "    x      : numpy.ndarry\n",
    "        Waiting time domain\n",
    "    amount_of_waiting_times : int\n",
    "        Total number of waiting time datapoints\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    # waiting_time_list validity\n",
    "    assert isinstance(waiting_time_list, (list, tuple)) and len(waiting_time_list) > 0, \"waiting_time_list must be a non-empty list or tuple.\"\n",
    "\n",
    "    # binsize validity\n",
    "    assert isinstance(binsize, (int, float)) and binsize > 0, f\"binsize must be a positive number it is currently {binsize}.\"\n",
    "\n",
    "    # Validate each waiting time array\n",
    "    for arr in waiting_time_list:\n",
    "        assert len(arr) > 0, \"Each waiting-time array must be non-empty.\"\n",
    "        assert np.all(np.array(arr) >= 0), \"Waiting times must be non-negative.\"\n",
    "    \n",
    "    # Check if there are gaps.\n",
    "    if (isinstance(time_gaps_start, np.ndarray) and len(time_gaps_start) == 1 and time_gaps_start[0] == 0):\n",
    "        \n",
    "        # If there are no gaps then the first index is the same as any other random index. So we choose the first index\n",
    "        waiting_times = waiting_time_list[0]\n",
    "        \n",
    "        # Calculate the total number of flares\n",
    "        amount_of_waiting_times = len(waiting_times)\n",
    "        \n",
    "    else:\n",
    "        # Combine all waiting times into one big array\n",
    "        waiting_times = np.concatenate(waiting_time_list)\n",
    "        \n",
    "        # Calculate the total number of flares\n",
    "        amount_of_waiting_times = len(waiting_times)\n",
    "        \n",
    "    # Find the maximum waiting time    \n",
    "    maximum_waiting_time = np.max(waiting_times)\n",
    "    \n",
    "    # Calculate the number of bins based on the maximum waiting time and binsize.\n",
    "    number_of_bins = int(maximum_waiting_time/binsize)\n",
    "    \n",
    "    # The probability density function is normalized to 1\n",
    "    counts, bin_edges = np.histogram(waiting_times, bins=number_of_bins, density=True)\n",
    "\n",
    "    # Compute bin centers\n",
    "    x = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    \n",
    "    # PDF must contain non-negative values\n",
    "    assert np.all(counts >= 0), \"PDF counts must be non-negative.\"\n",
    "\n",
    "    # counts are PDF values; to check normalization we integrate using bin widths.\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "    integral = np.sum(counts * bin_widths)\n",
    "    assert np.isclose(integral, 1.0, rtol=1e-2, atol=1e-2), f\"PDF does not integrate to 1 (got {integral}).\"\n",
    "\n",
    "    # Count array length matches expected number of bins\n",
    "    assert len(counts) == number_of_bins, \"Length of PDF counts must equal the number of bins.\"\n",
    "    \n",
    "    assert len(counts) == len(x), f\"Length of PDF counts must equal number domain points in x. Currently: {len(counts)},{len(x)}\"\n",
    "    \n",
    "    return counts, x, amount_of_waiting_times\n",
    "\n",
    "def cdf_maker(waiting_time_list, total_observing_time, number_of_flares_list, plot):\n",
    "    \"\"\"\n",
    "    Calculates the cumulative distribution function of the data and compares it \n",
    "    with the theoretical exponential CDF using the Kolmogorov–Smirnov test.\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    waiting_time_list : list of array-like\n",
    "        A list where each element contains waiting times (floats or ints).\n",
    "    total_observing_time : float\n",
    "        Total observing duration.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    D_values : list of float\n",
    "        KS D-statistics for each dataset.\n",
    "    p_values : list of float\n",
    "        KS p-values for each dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # waiting_time_list must be a list\n",
    "    assert isinstance(waiting_time_list, list), \"Precondition failed: waiting_time_list must be a list.\"\n",
    "    \n",
    "    assert isinstance(plot, bool), \"plot must be either True or False\"\n",
    "\n",
    "    # total_observing_time must be positive\n",
    "    assert total_observing_time > 0, \"total_observing_time must be > 0.\"\n",
    "\n",
    "    # length match\n",
    "    assert len(number_of_flares_list) == len(waiting_time_list), \"Precondition failed: number_of_flares_list and waiting_time_list must have same length.\"\n",
    "\n",
    "    # elements inside waiting_time_list must be sequences of numbers\n",
    "    for w in waiting_time_list:\n",
    "        assert hasattr(w, \"__iter__\"), \"Precondition failed: each element in waiting_time_list must be iterable.\"\n",
    "        assert all(val >= 0 for val in w), \"Precondition failed: waiting times must be non-negative.\"\n",
    "\n",
    "    D_values = []\n",
    "    p_values = []\n",
    "    \n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    for idx in range(len(waiting_time_list)):\n",
    "        \n",
    "        # Sort the indexed waiting time list\n",
    "        waiting_time_list_sorted = np.sort(waiting_time_list[idx])\n",
    "        \n",
    "        # Calculates the total number of waiting times present in the list\n",
    "        total = len(waiting_time_list_sorted)\n",
    "\n",
    "        # Empirical CDF\n",
    "        cdf = np.arange(1, total + 1) / total\n",
    "\n",
    "        # Mean flaring rate, lambda\n",
    "        theory_mean = number_of_flares_list[idx] / total_observing_time\n",
    "\n",
    "        # theoretical exponential CDF\n",
    "        if stacked == True:\n",
    "            # number of events (flares) that generated these pairwise differences\n",
    "            n_events = number_of_flares_list[idx]\n",
    "            assert n_events >= 2, \"Need at least 2 events for stacked pairwise differences.\"\n",
    "\n",
    "            # lags k = 1, ..., n_events-1\n",
    "            k_vals = np.arange(1, n_events)\n",
    "            # mixing weights w_k = 2 (n - k) / (n (n - 1))\n",
    "            weights = 2.0 * (n_events - k_vals) / (n_events * (n_events - 1))\n",
    "\n",
    "            def cdf_theory(x, lam=theory_mean, k_vals=k_vals, weights=weights):\n",
    "                # ensure numpy float array\n",
    "                x = np.asarray(x, dtype=float)\n",
    "\n",
    "                # mixture of Erlang(k, lam) CDFs, implemented via gamma.cdf\n",
    "                F = np.zeros_like(x, dtype=float)\n",
    "                for k, w in zip(k_vals, weights):\n",
    "                    # Erlang(k, lam) == Gamma(shape=k, scale=1/lam)\n",
    "                    F += w * gamma.cdf(x, a=k, scale=1.0 / lam)\n",
    "\n",
    "                return F\n",
    "        else:\n",
    "            def cdf_theory(x, lam=theory_mean):\n",
    "                return 1.0 - np.exp(-lam * x) \n",
    "\n",
    "        # Preforming the KS test\n",
    "        D, p_val = kstest(waiting_time_list_sorted, cdf_theory)\n",
    "        \n",
    "        # Appending all D and p values to their lists\n",
    "        D_values.append(D)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "        # Optional plot for index 1\n",
    "        if plot == True:\n",
    "            t = np.linspace(0, np.max(waiting_time_list_sorted), 100)\n",
    "            \n",
    "            if idx == 0:\n",
    "                ax.plot(t, cdf_theory(t)/np.max(cdf_theory(t)), label='CDF Theory',color='crimson',lw=2)\n",
    "                ax.plot(waiting_time_list_sorted, cdf, label='CDF Data',color='#003366',alpha=1)\n",
    "                \n",
    "            else:\n",
    "                ax.plot(waiting_time_list_sorted, cdf,color='#003366',alpha=len(waiting_time_list)/len(waiting_time_list)**2)\n",
    "                \n",
    "    if plot == True:\n",
    "        ax.set_xlim(0, np.max(waiting_time_list[0]))\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.set_xlabel('waiting time [day]')\n",
    "        ax.set_ylabel('Cumulative Density')\n",
    "        ax.set_title(f'CDF of MFR: {mfr:.3f}, PIR: {pir:.3f}',weight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "                \n",
    "    # Calculating the mean of the p and D values\n",
    "    D_values_mean = np.mean(D_values)\n",
    "    p_values_mean = np.mean(p_values)\n",
    "    \n",
    "    D_values_median = np.median(D_values)\n",
    "    p_values_median = np.median(p_values)\n",
    "    \n",
    "    # Returned lists must match the number of datasets\n",
    "    assert len(D_values) == len(waiting_time_list), \"Postcondition failed: D_values length mismatch.\"\n",
    "    assert len(p_values) == len(waiting_time_list), \"Postcondition failed: p_values length mismatch.\"\n",
    "\n",
    "    # D-statistics must be between 0 and 1\n",
    "    assert all(0 <= d <= 1 for d in D_values), \"Postcondition failed: D values must be in [0, 1].\"\n",
    "\n",
    "    # p-values must be between 0 and 1\n",
    "    assert all(0 <= p <= 1 for p in p_values), \"Postcondition failed: p-values must be in [0, 1].\"\n",
    "\n",
    "    return D_values, p_values, D_values_mean, p_values_mean, D_values_median, p_values_median, theory_mean\n",
    "\n",
    "\n",
    "def weighted_erlang_pdf(x, n_events, lam):\n",
    "    \"\"\"\n",
    "    Mixture of Erlang(k, lam) PDFs for stacked pairwise waiting times\n",
    "    from a Poisson process with n_events events in the interval.\n",
    "\n",
    "    x        : array-like of waiting times (bin centers)\n",
    "    n_events : integer >= 2 (number of flares/events)\n",
    "    lam      : Poisson rate (mean flaring rate)\n",
    "    \"\"\"\n",
    "    # Force clean float arrays/scalars\n",
    "    x_erlang = np.asarray(x, dtype=float)\n",
    "    lam = float(lam)\n",
    "\n",
    "    assert n_events >= 2, \"Need at least 2 events for stacked pairwise differences.\"\n",
    "\n",
    "    # lags k = 1, ..., n_events - 1\n",
    "    k_vals = np.arange(1, n_events, dtype=int)\n",
    "    # mixing weights: w_k = 2 (n - k) / (n (n - 1))\n",
    "    weights = 2.0 * (n_events - k_vals) / (n_events * (n_events - 1.0))\n",
    "    weights = np.asarray(weights, dtype=float)\n",
    "\n",
    "    pdf_erlang = np.zeros_like(x_erlang, dtype=float)\n",
    "\n",
    "    for k, w in zip(k_vals, weights):\n",
    "        # Erlang(k, lam) PDF:\n",
    "        # f_k(x) = lam^k x^{k-1} e^{-lam x} / (k-1)!\n",
    "        term = w * (lam**k) * (x_erlang**(k - 1)) * np.exp(-lam * x_erlang) / math.factorial(k - 1)\n",
    "        term = np.asarray(term, dtype=float)  # make sure it's float\n",
    "        pdf_erlang += term\n",
    "\n",
    "    return pdf_erlang\n",
    "\n",
    "\n",
    "def pdf_fitter(pdf, x, amount_of_waiting_times, theory_mean, number_of_flares_list, plot):\n",
    "    \"\"\"\n",
    "    Compute a theoretical PDF (stacked or non-stacked) and optionally plot it.\n",
    "\n",
    "    This function compares empirical PDF data (`pdf`) to a theoretical model.\n",
    "    It supports two modes:\n",
    "    - **Stacked mode** (`stacked = True`): Uses a weighted Erlang distribution.\n",
    "    - **Non-stacked mode** (`stacked = False`): Uses an exponential distribution.\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    pdf : array-like\n",
    "        The empirical probability density values.\n",
    "    x : array-like\n",
    "        The x-values (waiting times) corresponding to `pdf`.\n",
    "    amount_of_waiting_times : int\n",
    "        Number of pairwise waiting times (N_pairs) if using the stacked model.\n",
    "    theory_mean : float\n",
    "        Mean rate parameter λ for the theoretical distribution.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    pdf_fitted : ndarray\n",
    "        The theoretical PDF evaluated at x.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Preconditions -------------------------\n",
    "    assert len(pdf) == len(x), f\"`pdf` and `x` must be same length: {len(pdf)},{len(x)}\"\n",
    "    assert theory_mean > 0, \"`theory_mean` must be positive.\"\n",
    "    \n",
    "    assert isinstance(plot, bool), \"plot must be either True or False\"\n",
    "\n",
    "    if stacked is True:\n",
    "        # Weighted Erlang Model\n",
    "        pdf_fitted = weighted_erlang_pdf(x, number_of_flares_list[0], theory_mean)\n",
    "\n",
    "    else:\n",
    "        # Exponential model\n",
    "        pdf_fitted = theory_mean * np.exp(-x * theory_mean)\n",
    "\n",
    "    # --- Plotting ------------------------------\n",
    "    if plot is True:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.plot(x, pdf, color='#003366', label='PDF Data', lw=2)\n",
    "        ax.plot(x, pdf_fitted, color='crimson', label='PDF Theory', lw=2)\n",
    "        PDF_error_plotter(x,pdf_fitted,np.std(pdf_fitted),ax)\n",
    "        ax.set_xlim(np.min(x), np.max(x))\n",
    "        ax.set_ylim(0, 1.1 * np.maximum(np.max(pdf), np.max(pdf_fitted)))\n",
    "        ax.set_xlabel('waiting time [day]')\n",
    "        ax.set_ylabel('Probability Density')\n",
    "        ax.set_title(f'PDF of MFR: {mfr:.3f}, PIR: {pir:.3f}', weight='bold')\n",
    "        ax.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Postconditions -------------------------\n",
    "    assert len(pdf) == len(pdf_fitted),f\"pdf and pdf_fitted lengths differ: {len(pdf)},{len(pdf_fitted)}\"\n",
    "    \n",
    "    return pdf_fitted\n",
    "\n",
    "\n",
    "def PDF_error_plotter(x,function,function_std,ax):\n",
    "    \"\"\"Removes a lot of junk code in each plot with error bands.\n",
    "    \n",
    "    Input:\n",
    "    x: Domain\n",
    "    function: The function on which the standard deviations are applied\n",
    "    function_std: the standard deviation of the function\n",
    "    \n",
    "    Output:\n",
    "    Adds coloring of the standard deviation bounds on the inputted function\"\"\"\n",
    "    \n",
    "    \n",
    "    # Define ±kσ bands (k = 1..4)\n",
    "    levels = [1, 2, 3]\n",
    "    band_colors = {\n",
    "                1: '#1f77b4',  # deep blue\n",
    "                2: '#2ca02c',  # fresh green\n",
    "                3: '#ff7f0e'  # warm orange\n",
    "    }\n",
    "    # Darker fill for narrower bands\n",
    "    band_alphas = {1: 0.55, 2: 0.45, 3: 0.35}\n",
    "\n",
    "    # Shaded mean ± k·std bands (plot widest first so narrowest stays on top)\n",
    "\n",
    "    for k in sorted(band_colors.keys(), reverse=True):\n",
    "        lower = function - k * function_std\n",
    "        upper = function + k * function_std\n",
    "        ax.fill_between(x, lower, upper, color=band_colors[k], alpha=band_alphas[k], linewidth=0, label=f'PDF Theory ± {k}σ')      \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_ks_results(D_values, p_values):\n",
    "    \"\"\"\n",
    "    Plot histograms and scatter plot for KS-test results.\n",
    "    Includes medians and shaded ±kσ bands (k=1..3) for D_values and log10(p_values).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    D_values : array-like\n",
    "        Array of D statistic values from the KS-test.\n",
    "    p_values : array-like\n",
    "        Array of p-values from the KS-test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    D_values = np.asarray(D_values)\n",
    "    p_values = np.asarray(p_values)\n",
    "\n",
    "    # Guard against non-positive p-values in log10\n",
    "    mask = p_values > 0\n",
    "    if not np.all(mask):\n",
    "        print(\"Warning: some p-values <= 0 were removed before taking log10.\")\n",
    "    D_values = D_values[mask]\n",
    "    p_values = p_values[mask]\n",
    "\n",
    "    # ----- config for ±kσ bands -----\n",
    "    levels = [1, 2, 3]\n",
    "    band_colors = {\n",
    "        1: '#1f77b4',  # deep blue\n",
    "        2: '#2ca02c',  # fresh green\n",
    "        3: '#ff7f0e'   # orange\n",
    "    }\n",
    "    band_alphas = {1: 0.55, 2: 0.45, 3: 0.35}\n",
    "\n",
    "    # Create figure and 3 subplots\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Histogram of D_values\n",
    "    median_D = np.median(D_values)\n",
    "    mean_D   = np.mean(D_values)\n",
    "    std_D    = np.std(D_values)\n",
    "\n",
    "    ax[0].hist(D_values, bins=30, color='#003366', alpha=1, edgecolor='darkblue')\n",
    "    ax[0].set_xlabel('D value')\n",
    "    ax[0].set_ylabel('Frequency')\n",
    "    ax[0].set_title('Distribution of D values from KS-test', weight=\"bold\")\n",
    "\n",
    "    # Shaded ±kσ bands around the *median* (matching docstring)\n",
    "    band_patches = []\n",
    "    for k in sorted(levels, reverse=True):\n",
    "        lower = median_D - k * std_D\n",
    "        upper = median_D + k * std_D\n",
    "        ax[0].axvspan(lower, upper, color=band_colors[k], alpha=band_alphas[k], zorder=0)\n",
    "        band_patches.append(mpatches.Patch(color=band_colors[k], alpha=band_alphas[k], label=f'median ± {k}σ'))\n",
    "\n",
    "    # Median line\n",
    "    median_line = ax[0].axvline(median_D, color='crimson', linestyle='--', linewidth=2,label=f'Median = {median_D:.3f}')\n",
    "\n",
    "    # Legend\n",
    "    ax[0].legend(handles=[median_line, *band_patches], loc='upper right')\n",
    "\n",
    "    # Histogram of log10(p_values)\n",
    "    log_p = np.log10(p_values)\n",
    "    median_logp = np.median(log_p)\n",
    "    mean_logp   = np.mean(log_p)\n",
    "    std_logp    = np.std(log_p)\n",
    "\n",
    "    ax[1].hist(log_p, bins=30, color='#003366', alpha=1, edgecolor='darkblue')\n",
    "    ax[1].set_xlabel('log10(p)')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].set_title('Distribution of log10(p-values) from KS-test', weight=\"bold\")\n",
    "\n",
    "    band_patches2 = []\n",
    "    for k in sorted(levels, reverse=True):\n",
    "        lower = median_logp - k * std_logp\n",
    "        upper = median_logp + k * std_logp\n",
    "        ax[1].axvspan(lower, upper, color=band_colors[k], alpha=band_alphas[k], zorder=0)\n",
    "        band_patches2.append(mpatches.Patch(color=band_colors[k], alpha=band_alphas[k], label=f'median ± {k}σ'))\n",
    "\n",
    "    median_logp_line = ax[1].axvline(median_logp, color='crimson', linestyle='--', linewidth=2,label=f'Median = {median_logp:.3f}')\n",
    "\n",
    "    ax[1].legend(handles=[median_logp_line, *band_patches2], loc='upper right')\n",
    "\n",
    "    # Optionally invert x-axis so high p-values (less significant) appear on the left\n",
    "    ax[1].invert_xaxis()\n",
    "\n",
    "    # Scatter: D_values vs log10(p_values)\n",
    "    ax[2].set_xlabel('D value')\n",
    "    ax[2].set_ylabel('log10(p)')\n",
    "    ax[2].set_title('D vs log10(p)', weight=\"bold\")\n",
    "\n",
    "    # Shaded vertical (D) and horizontal (log10 p) ±kσ bands around medians\n",
    "    for k in sorted(levels, reverse=True):\n",
    "        ax[2].axvspan(median_D - k*std_D, median_D + k*std_D,color=band_colors[k], alpha=band_alphas[k], zorder=1)\n",
    "        ax[2].axhspan(median_logp - k*std_logp, median_logp + k*std_logp,color=band_colors[k], alpha=band_alphas[k], zorder=1)\n",
    "\n",
    "    # Scatter points (on top of bands)\n",
    "    ax[2].scatter(D_values, log_p, color='#003366', alpha=1,edgecolor='darkblue', zorder=3, s=20)\n",
    "    ax[2].scatter(median_D, median_logp,color='crimson',label=f'Medians: D = {median_D:.3f}, log10(p) = {median_logp:.3f}',zorder=4,s=60)\n",
    "\n",
    "    ax[2].legend(loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "    \n",
    "    \n",
    "def statistics(waiting_time_list, t_gaps_start, total_observing_time, number_of_flares_list, plot, stacked):\n",
    "    \n",
    "    if stacked == True:\n",
    "        binsize = total_observing_time / 100\n",
    "    else:\n",
    "        binsize = total_observing_time / 500\n",
    "    \n",
    "    pdf, x, amount_of_waiting_times = pdf_prepper(waiting_time_list, t_gaps_start, binsize)\n",
    "    \n",
    "    D_values, p_values, D_values_mean, p_values_mean, D_values_median, p_values_median, theory_mean =  cdf_maker(waiting_time_list,total_observing_time, number_of_flares_list, plot)\n",
    "\n",
    "    pdf_fitted = pdf_fitter(pdf, x, amount_of_waiting_times, theory_mean, number_of_flares_list, plot)\n",
    "    \n",
    "    if plot == True:\n",
    "        plot_ks_results(D_values, p_values)\n",
    "        \n",
    "    return D_values_mean, p_values_mean, D_values_median, p_values_median, D_values, p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2457b",
   "metadata": {},
   "source": [
    "### Star:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b161414",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_name = 'twa7'\n",
    "flare_data_path = \"../Data & Results/twa7/twa7_processed_data/twa7_flares.csv\"\n",
    "observation_time_path = \"../Data & Results/twa7/twa7_processed_data/twa7_times.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff2154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_data = pd.read_csv(flare_data_path)\n",
    "flaring_times = flare_data['tstart']\n",
    "\n",
    "observational_times = pd.read_csv(observation_time_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3243081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        2282.084517\n",
      "1        2282.085906\n",
      "2        2282.087295\n",
      "3        2282.088684\n",
      "4        2282.090073\n",
      "            ...     \n",
      "15512    2305.987248\n",
      "15513    2305.988637\n",
      "15514    2305.990026\n",
      "15515    2305.991415\n",
      "15516    2305.992804\n",
      "Name: time, Length: 15517, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(observational_times['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d559edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_gaps(time):\n",
    "    \"\"\"\n",
    "    Detects and characterizes significant temporal gaps (> 0.1 days) in sorted time arrays.\n",
    "\n",
    "    This function:\n",
    "      - Scans each sorted time array for consecutive points separated by more than 0.1 days\n",
    "      - Records the index where each gap begins\n",
    "      - Stores the gap sizes along with their start and end times\n",
    "      - Prints a summary of the number of gaps, and the smallest and largest gap sizes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : list of array-like\n",
    "        A list containing one or more sorted 1D arrays of time values (in days).\n",
    "        Each array must be monotonically increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    time_index : numpy.ndarray\n",
    "        Indices where gaps larger than 0.1 days begin.\n",
    "    time_gaps : numpy.ndarray\n",
    "        Sizes of detected gaps (in days).\n",
    "    start_time : numpy.ndarray\n",
    "        Start time of each detected gap.\n",
    "    end_time : numpy.ndarray\n",
    "        End time of each detected gap.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing the lists\n",
    "    time_index, time_gaps, start_time, end_time = [],[],[],[]\n",
    "\n",
    "    # Looping over all time data \n",
    "    for t_array in time: \n",
    "        # calculating the base timesteps per dataset\n",
    "        timestep_0 = t_array[1] - t_array[0]\n",
    "        \n",
    "        # Looping over each dataset\n",
    "        for i in range(len(t_array)):\n",
    "            if i == len(t_array)-1:\n",
    "                if len(time_index) == 0:\n",
    "                    continue\n",
    "                print(f\"Amount of gaps: {len(time_index)}, Smallest timegap: {np.min(time_gaps):.2f} days. Longest timegap: {np.max(time_gaps):.2f} days.\")\n",
    "            else: \n",
    "                if t_array[i+1] - t_array[i] > 0.1: # If the gap is larger than 0.1 day, denote the index and gap size\n",
    "                    time_index.append(i)\n",
    "                    time_gaps.append(t_array[i+1] - t_array[i])\n",
    "                    start_time.append(t_array[i])\n",
    "                    end_time.append(t_array[i+1])\n",
    "        \n",
    "    return np.array(time_index), np.array(time_gaps), np.array(start_time), np.array(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e598e5a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m time_index, time_gaps, start_time, end_time \u001b[38;5;241m=\u001b[39m Find_gaps(observational_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m, in \u001b[0;36mFind_gaps\u001b[0;34m(time)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Looping over all time data \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_array \u001b[38;5;129;01min\u001b[39;00m time: \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# calculating the base timesteps per dataset\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     timestep_0 \u001b[38;5;241m=\u001b[39m t_array[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m t_array[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Looping over each dataset\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t_array)):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "time_index, time_gaps, start_time, end_time = Find_gaps(observational_times['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a52a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
